{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the data and creating train,test and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "from io import open\n",
    "file=open('hi_hdtb-ud-train.conllu','r',encoding='utf-8')\n",
    "ud_files=[]\n",
    "for tokenlist in parse_incr(file):\n",
    "    ud_files.append(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(ud_files):\n",
    "    bank=[]\n",
    "    for sentence in ud_files:\n",
    "        tokens=[]\n",
    "        tags=[]\n",
    "\n",
    "        for token in sentence:\n",
    "            tokens.append(token['form'])\n",
    "            tags.append(token['upostag'])\n",
    "\n",
    "        bank.append((tokens,tags))\n",
    "    return bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(bank):\n",
    "    X,y=[],[]\n",
    "    for index in range(len(bank)):\n",
    "        X.append(bank[index][0])\n",
    "        y.append(bank[index][1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=separate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('hi_hdtb-ud-test.conllu','r',encoding='utf-8')\n",
    "ud_files=[]\n",
    "for tokenlist in parse_incr(file):\n",
    "    ud_files.append(tokenlist)\n",
    "test=dataset(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('hi_hdtb-ud-dev.conllu','r',encoding='utf-8')\n",
    "ud_files=[]\n",
    "for tokenlist in parse_incr(file):\n",
    "    ud_files.append(tokenlist)\n",
    "dev=dataset(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest,ytest=separate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev,ydev=separate(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating function to extract features and using it on the train, test and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, index):\n",
    "    return{\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'next_word':sentence[index+1] if index<len(sentence)-1 else '',\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=[]\n",
    "for index in range(len(X)):\n",
    "    arrange=[]\n",
    "    for i in range(len(X[index])):\n",
    "        arrange.append(extract_features(X[index],i))\n",
    "    xtrain.append(arrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest=[]\n",
    "for index in range(len(Xtest)):\n",
    "    arrange=[]\n",
    "    for i in range(len(Xtest[index])):\n",
    "        arrange.append(extract_features(Xtest[index],i))\n",
    "    xtest.append(arrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdev=[]\n",
    "for index in range(len(Xdev)):\n",
    "    arrange=[]\n",
    "    for i in range(len(Xdev[index])):\n",
    "        arrange.append(extract_features(Xdev[index],i))\n",
    "    xdev.append(arrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing sklearn_crf suite and initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "\n",
    "hindi_crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.20,\n",
    "    c2=0.3,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "print(\"Started training \")\n",
    "hindi_crf.fit(xtrain, y)\n",
    "print(\"Finished training \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "print(\"##nltk##\")\n",
    "y_pred = hindi_crf.predict(xtest)\n",
    "print(\"F1 score on Test Data\")\n",
    "print(metrics.flat_f1_score(ytest, y_pred,average='weighted',labels=hindi_crf.classes_))\n",
    "\n",
    "# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\n",
    "print(\"Class wise score:\")\n",
    "print(metrics.flat_classification_report(\n",
    "    ytest, y_pred, labels=hindi_crf.classes_, digits=3\n",
    "))\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "y_pred = model.predict(xdev)\n",
    "print(\"F1 score on Dev Data\")\n",
    "print(metrics.flat_f1_score(ydev, y_pred,average='weighted',labels=hindi_crf.classes_))\n",
    "print(metrics.flat_accuracy_score(ydev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='पत्तेदार सब्जियां आपके स्वास्थ्य के लिए अच्छी होती हैं ।'\n",
    "list1=[]\n",
    "list1.append(sentence.split())\n",
    "xtesting=[]\n",
    "for index in range(len(list1)):\n",
    "    arrange=[]\n",
    "    for i in range(len(list1[index])):\n",
    "        arrange.append(extract_features(list1[index],i))\n",
    "    xtesting.append(arrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = hindi_crf.predict(xtesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
