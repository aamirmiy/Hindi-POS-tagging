{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "from io import open\n",
    "file=open('hi_hdtb-ud-train.conllu','r',encoding='utf-8')\n",
    "ud_files=[]\n",
    "for tokenlist in parse_incr(file):\n",
    "    ud_files.append(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(ud_files):\n",
    "    bank=[]\n",
    "    for sentence in ud_files:\n",
    "        tokens=[]\n",
    "        tags=[]\n",
    "\n",
    "        for token in sentence:\n",
    "            tokens.append(token['form'])\n",
    "            tags.append(token['upostag'])\n",
    "\n",
    "        bank.append((tokens,tags))\n",
    "    return bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(bank):\n",
    "    X,y=[],[]\n",
    "    for index in range(len(bank)):\n",
    "        X.append(bank[index][0])\n",
    "        y.append(bank[index][1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=separate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('hi_hdtb-ud-test.conllu','r',encoding='utf-8')\n",
    "ud_files=[]\n",
    "for tokenlist in parse_incr(file):\n",
    "    ud_files.append(tokenlist)\n",
    "test = dataset(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('hi_hdtb-ud-dev.conllu','r',encoding='utf-8')\n",
    "ud_files=[]\n",
    "for tokenlist in parse_incr(file):\n",
    "    ud_files.append(tokenlist)\n",
    "dev = dataset(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest,ytest=separate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdev,ydev=separate(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = set()\n",
    "tag_count = {}\n",
    "word_set = set()\n",
    "\n",
    "def transition_count(X,y): ## Function for calculating the frequency of tag combinations.\n",
    "    global tag_list # example: Noun-verb , noun-adjective, verb-adjective etc.\n",
    "    global word_set\n",
    "    transition_dict = {} # initialize dictionary containing tag combinations as key and value as frequency.\n",
    "    global tag_count\n",
    "    for v in range(len(X)):\n",
    "        previous=\"start\"\n",
    "        for data in range(len(X[v])):\n",
    "            i=X[v][data]\n",
    "            word = i\n",
    "            word_set.add(word.lower())\n",
    "            tag = y[v][data]\n",
    "            tag_list.add(tag)\n",
    "\n",
    "            if tag in tag_count:\n",
    "                tag_count[tag]+=1\n",
    "            else:\n",
    "                tag_count[tag] = 1\n",
    "\n",
    "\n",
    "            if (previous + \"~tag~\" + tag) in transition_dict: #if tag combination is already present then increment count by 1\n",
    "                    transition_dict[previous + \"~tag~\" + tag] += 1\n",
    "                    previous = tag\n",
    "            else:  # if tag combination not present then initialize count for that combination to 1.\n",
    "                    transition_dict[previous + \"~tag~\" + tag] = 1\n",
    "                    previous = tag\n",
    "\n",
    "    return transition_dict,tag_count,tag_list,word_set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_m,tag_count,tag_list,word_set = transition_count(X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_probability(X,y):\n",
    "    #count_dict = transition_count(X,y)\n",
    "    count_dict = transmission_m\n",
    "    prob_dict = {}\n",
    "    for key in count_dict:\n",
    "        den = 0\n",
    "        val = key.split(\"~tag~\")[0]\n",
    "        # Probabilty of a tagA to be followed by tagB out of all possible tags # \n",
    "        for key_2 in count_dict:\n",
    "            if key_2.split(\"~tag~\")[0] == val:\n",
    "                den += count_dict[key_2]\n",
    "        prob_dict[key] = Decimal(count_dict[key])/(den)\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_smoothing(X,y):\n",
    "    transition_prob = transition_probability(X,y)\n",
    "    for tag in tag_list:\n",
    "        # if a tag does not occur as a start tag, then set its probability to be a start tag to minimum value #\n",
    "        if \"start\" + tag not in  transition_prob:\n",
    "            transition_prob[(\"start\" + \"~tag~\" + tag)] = Decimal(1) / Decimal(len(word_set) + tag_count[tag])\n",
    "    for tag1 in tag_list:\n",
    "        for tag2 in tag_list:\n",
    "        # if a particular tag combination does not exist in the dictionary, we set its probability to minimum#\n",
    "            if (tag1 +\"~tag~\" + tag2) not in transition_prob:\n",
    "                transition_prob[(tag1+\"~tag~\"+tag2)] = Decimal(1)/Decimal(len(word_set) + tag_count[tag1])\n",
    "    return transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_count(X,y):   #FUNCTION FOR MAPPING THE WORDS WITH CORRESPONDING TAGS AND CALCULATE FREQUENCY OF EACH WORD AND TAG COMBINATION\n",
    "    count_word = {}\n",
    "    for v in range(len(X)):\n",
    "        for data in range(len(X[v])):\n",
    "            i = X[v][data]\n",
    "            word = i\n",
    "            tag = y[v][data]\n",
    "            # map the words in the training set to their tagged POS #\n",
    "            if word.lower() + \"/\" + tag in count_word:\n",
    "                count_word[word.lower() + \"/\" + tag] +=1\n",
    "            else:\n",
    "                count_word[word.lower() + \"/\" + tag] = 1\n",
    "    return count_word  #RETURN DICTIONARY CONATINING THE EMISSION COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_probability(X,y):\n",
    "    global tag_count\n",
    "    word_count = emission_count(X,y)\n",
    "    emission_prob_dict = {}\n",
    "    # calculate probability of a word to be a certain Tag out of all the possible tags that it can be #\n",
    "    for key in word_count:\n",
    "        emission_prob_dict[key] = Decimal(word_count[key])/tag_count[key.split(\"/\")[-1]]\n",
    "    return emission_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = transition_smoothing(X,y)\n",
    "emission_model = emission_probability(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence, tag_list, transition_prob, emission_prob,tag_count, word_set):\n",
    "    global tag_set\n",
    "    # Get words from each sentence \n",
    "    sentence = sentence.strip(\"\\n\")\n",
    "    word_list = sentence.split(\" \")\n",
    "    current_prob = {}\n",
    "    for tag in tag_list:\n",
    "        # transition probability set to minimum\n",
    "        tp = Decimal(0)\n",
    "        # Emission probability set to minimum\n",
    "        em = Decimal(0)\n",
    "        # Storing the probability of every tag to be starting tag #\n",
    "        if \"start~tag~\"+tag in transition_prob:\n",
    "            tp = Decimal(transition_prob[\"start~tag~\"+tag])\n",
    "        # Check for word in training data. If present, check the probability of the first word to be of given tag#\n",
    "        if word_list[0].lower() in word_set:\n",
    "            if (word_list[0].lower()+\"/\"+tag) in emission_prob:\n",
    "                em = Decimal(emission_prob[word_list[0].lower()+\"/\"+tag])\n",
    "                # Storing probability of current combination of tp and em #\n",
    "                current_prob[tag] = tp * em\n",
    "         # Check for word in training data. If absent then probability is just tp# \n",
    "        else:\n",
    "            em = Decimal(1) /(tag_count[tag] +len(word_set))\n",
    "            current_prob[tag] = tp\n",
    "\n",
    "    if len(word_list) == 1:\n",
    "        # Return max path if only one word in sentence #\n",
    "        max_path = max(current_prob, key=current_prob.get)\n",
    "        return max_path\n",
    "    else:\n",
    "        # Tracking from second word to last word #\n",
    "        for i in range(1, len(word_list)):\n",
    "            previous_prob = current_prob\n",
    "            current_prob = {}\n",
    "            locals()['dict{}'.format(i)] = {}\n",
    "            previous_tag = \"\"\n",
    "            for tag in tag_list:\n",
    "                if word_list[i].lower() in word_set:\n",
    "                    if word_list[i].lower()+\"/\"+tag in emission_prob:\n",
    "                        em = Decimal(emission_prob[word_list[i].lower()+\"/\"+tag])\n",
    "                        # Find the maximum probability using previous node's(tp*em)[i.e probability of reaching to the previous node] * tp * em (Bigram Model) #\n",
    "                        max_prob, previous_state = max((Decimal(previous_prob[previous_tag]) * Decimal(transition_prob[previous_tag + \"~tag~\" + tag]) * em, previous_tag) for previous_tag in previous_prob)\n",
    "                        current_prob[tag] = max_prob\n",
    "                        locals()['dict{}'.format(i)][previous_state + \"~\" + tag] = max_prob\n",
    "                        previous_tag = previous_state\n",
    "                else:\n",
    "                    em = Decimal(1) /(tag_count[tag] +len(word_set))\n",
    "                    max_prob, previous_state = max((Decimal(previous_prob[previous_tag]) * Decimal(transition_prob[previous_tag+\"~tag~\"+tag]) * em, previous_tag) for previous_tag in previous_prob)\n",
    "                    current_prob[tag] = max_prob\n",
    "                    locals()['dict{}'.format(i)][previous_state + \"~\" + tag] = max_prob\n",
    "                    previous_tag = previous_state\n",
    "\n",
    "            # if last word of sentence, then return path dicts of all words #\n",
    "            if i == len(word_list)-1:\n",
    "                max_path = \"\"\n",
    "                last_tag = max(current_prob, key=current_prob.get)\n",
    "                max_path = max_path + last_tag + \" \" + previous_tag\n",
    "                for j in range(len(word_list)-1,0,-1):\n",
    "                    for key in locals()['dict{}'.format(j)]:\n",
    "                        data = key.split(\"~\")\n",
    "                        if data[-1] == previous_tag:\n",
    "                            max_path = max_path + \" \" +data[0]\n",
    "                            previous_tag = data[0]\n",
    "                            break\n",
    "                result = max_path.split()\n",
    "                result.reverse()\n",
    "                return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='यह एशिया की सबसे बड़ी मस्जिदों में से एक है ।'\n",
    "path = viterbi_algorithm(sent, tag_list, transition_model, emission_model,tag_count, word_set)\n",
    "word = sent.split(\" \")\n",
    "tag = path.split(\" \")\n",
    "for j in range(0,len(word)):\n",
    "    if(j==len(word)-1):\n",
    "        print(word[j] + \"/\" + tag[j]+ u'\\n')\n",
    "    else:\n",
    "        print(word[j] + \"/\" + tag[j] + \" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
